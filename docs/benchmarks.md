# Benchmarks

HippocampAI includes a reproducible benchmark suite for measuring performance.

## Quick Start

Run benchmarks with a single command:

```bash
# Full benchmark suite
python -m bench.run_benchmarks

# Quick benchmark (reduced iterations)
python -m bench.run_benchmarks --quick

# Custom configuration
python -m bench.run_benchmarks \
  --api-url http://localhost:8000 \
  --ingestion 200 \
  --retrieval 100 \
  --context 50
```

## What's Measured

### Ingestion Throughput

Measures how fast memories can be added to the system.

- **Metric:** Memories per second
- **Method:** Sequential memory insertion
- **Data:** Synthetic memories (facts, preferences, events)

### Retrieval Latency

Measures search/recall performance.

- **Metrics:** p50, p95, p99 latency in milliseconds
- **Method:** Hybrid retrieval (vector + BM25) with reranking
- **Data:** Realistic search queries

### Context Assembly

Measures the automated context pack generation.

- **Metrics:** Latency and throughput
- **Method:** Full pipeline (retrieve → rerank → dedupe → compress)
- **Data:** Varied queries with token budget constraints

## Running Benchmarks

### Prerequisites

1. HippocampAI API running:
   ```bash
   docker compose up -d hippocampai
   ```

2. Python environment with dependencies:
   ```bash
   pip install -e ".[all]"
   ```

### Command Line Options

```
python -m bench.run_benchmarks [options]

Options:
  --api-url URL       HippocampAI API URL (default: http://localhost:8000)
  --api-key KEY       API key for authentication
  --ingestion N       Number of memories to ingest (default: 100)
  --retrieval N       Number of retrieval queries (default: 50)
  --context N         Number of context assembly calls (default: 20)
  --output DIR        Output directory (default: bench/results)
  --quick             Run quick benchmark (reduced iterations)
  --verbose           Enable verbose logging
```

### Environment Variables

```bash
export HIPPOCAMPAI_API_URL=http://localhost:8000
export HIPPOCAMPAI_API_KEY=your_api_key
```

## Output

Results are saved in two formats:

### JSON (machine-readable)

```json
{
  "name": "HippocampAI Benchmarks",
  "timestamp": "2024-12-24T10:30:00",
  "system_info": {
    "python_version": "3.12.0",
    "platform": "macOS-14.0-arm64"
  },
  "results": [
    {
      "name": "Ingestion",
      "operations": 100,
      "ops_per_second": 45.2,
      "latency_p50_ms": 18.5,
      "latency_p95_ms": 32.1,
      "latency_p99_ms": 45.8
    }
  ]
}
```

### Markdown (human-readable)

```markdown
# Benchmark Results

| Benchmark | Ops/sec | P50 (ms) | P95 (ms) |
|-----------|---------|----------|----------|
| Ingestion | 45.2    | 18.5     | 32.1     |
| Retrieval | 28.7    | 25.3     | 48.2     |
```

## Interpreting Results

### Ingestion

- **Good:** > 50 ops/sec
- **Acceptable:** 20-50 ops/sec
- **Needs optimization:** < 20 ops/sec

Factors affecting ingestion:
- Embedding model speed
- Vector database write performance
- Network latency

### Retrieval

- **Good:** p95 < 100ms
- **Acceptable:** p95 100-500ms
- **Needs optimization:** p95 > 500ms

Factors affecting retrieval:
- Vector index size and configuration
- Reranker model complexity
- Number of candidates (top_k)

### Context Assembly

- **Good:** p95 < 500ms
- **Acceptable:** p95 500-2000ms
- **Needs optimization:** p95 > 2000ms

Factors affecting context assembly:
- Token budget (larger = more processing)
- Number of candidate memories
- Summarization (if enabled)

## Synthetic Data

The benchmark uses synthetic data generated by `bench/data_generator.py`:

- **Facts:** "Alice works at Acme Corp as an Engineer"
- **Preferences:** "User prefers dark mode"
- **Events:** "Meeting with Bob about project deadline"

This ensures reproducible results without requiring external datasets.

## Customizing Benchmarks

### Adding Custom Benchmarks

```python
from bench.runner import run_benchmark, BenchmarkResult

def my_custom_operation():
    # Your operation here
    pass

result = run_benchmark(
    name="Custom Benchmark",
    operation=my_custom_operation,
    iterations=100,
    warmup=5,
)
```

### Using Custom Data

```python
from bench.data_generator import generate_memories

# Generate 1000 memories for 10 users
memories = list(generate_memories(
    count=1000,
    num_users=10,
))
```

## CI Integration

Add to your CI pipeline:

```yaml
# .github/workflows/benchmark.yml
name: Benchmarks

on:
  push:
    branches: [main]
  schedule:
    - cron: '0 0 * * 0'  # Weekly

jobs:
  benchmark:
    runs-on: ubuntu-latest
    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install -e ".[all]"

      - name: Run benchmarks
        run: python -m bench.run_benchmarks --quick

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench/results/
```

## Troubleshooting

### Connection Refused

Ensure the API is running:
```bash
docker compose up -d hippocampai
curl http://localhost:8000/healthz
```

### Slow Ingestion

Check embedding model:
```bash
# Use a smaller/faster model
export EMBED_MODEL=BAAI/bge-small-en-v1.5
```

### High Retrieval Latency

Tune HNSW parameters:
```bash
export HNSW_M=32
export EF_CONSTRUCTION=128
```

### Out of Memory

Reduce batch sizes:
```bash
export EMBED_BATCH_SIZE=16
```
